clear; 
clc;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Diabetes dataset 

load('diabetes.mat');

lambda = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10];

%For each lambda, use ridge regression function to get the w which can
%minize the value of object function. And get the mean squared error at the
%same time.

[a, b] = size(x_train);

outcome = ones(b, length(lambda));
train_mse = zeros(1, length(lambda));
test_mse = zeros(1, length(lambda));

for i = 1 : length(lambda)
    w = RidgeRegress(x_train, y_train, lambda(i));
    outcome(:, i) = w;
    y_tt = x_train * w;
    y_pred = x_test * w;
    train_mse(i) = MSE(y_tt, y_train);
    test_mse(i) = MSE(y_pred, y_test);
end



  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Cross-validation Part

%Devide training dataset
x_1 = x_train(1:50, :);
x_2 = x_train(51:100, :);
x_3 = x_train(101:150, :);
x_4 = x_train(151:200, :);
x_5 = x_train(201:242, :);

y_1 = y_train(1:50, :);
y_2 = y_train(51:100, :);
y_3 = y_train(101:150, :);
y_4 = y_train(151:200, :);
y_5 = y_train(201:242, :);

% Construct training set and C-V set;
% Left one out cross validation, so there are 5 sets

train1_x = [x_1; x_2; x_3; x_4];
train1_y = [y_1; y_2; y_3; y_4];
CV1_x = x_5;
CV1_y = y_5;

train2_x = [x_1; x_2; x_3; x_5];
train2_y = [y_1; y_2; y_3; y_5];
CV2_x = x_4;
CV2_y = y_4;

train3_x = [x_2; x_3; x_4; x_5];
train3_y = [y_2; y_3; y_4; y_5];
CV3_x = x_1;
CV3_y = y_1;

train4_x = [x_1; x_3; x_4; x_5];
train4_y = [y_1; y_3; y_4; y_5];
CV4_x = x_2;
CV4_y = y_2;

train5_x = [x_1; x_2; x_4; x_5];
train5_y = [y_1; y_2; y_4; y_5];
CV5_x = x_3;
CV5_y = y_3;

%cross validation error matrix
CV_err = zeros(5, length(lambda));

%5 times cross validation training, and getting cross validation error;

    for j = 1 : length(lambda)
       w_cv = RidgeRegress(train1_x, train1_y, lambda(j));
       CV_pred =  CV1_x * w_cv ;
       CV_err(1, j) = MSE(CV_pred, CV1_y);
    end
    
    for j = 1 : length(lambda)
       w_cv = RidgeRegress(train2_x, train2_y, lambda(j));
       CV_pred =  CV2_x * w_cv ;
       CV_err(2, j) = MSE(CV_pred, CV2_y);
    end
    
    for j = 1 : length(lambda)
       w_cv = RidgeRegress(train3_x, train3_y, lambda(j));
       CV_pred = CV3_x * w_cv ;
       CV_err(3, j) = MSE(CV_pred, CV3_y);
    end
    
    for j = 1 : length(lambda)
       w_cv = RidgeRegress(train4_x, train4_y, lambda(j));
       CV_pred =  CV4_x * w_cv ;
       CV_err(4, j) = MSE(CV_pred, CV4_y);
    end
    
    for j = 1 : length(lambda)
       w_cv = RidgeRegress(train5_x, train5_y, lambda(j));
       CV_pred =  CV5_x * w_cv ;
       CV_err(5, j) = MSE(CV_pred, CV5_y);
    end
    
Total_cv_err = sum(CV_err);
 
%find the best regualrization parameter, and use the whole training set to
%train the data again, compute the error in testing set.

flag = find(Total_cv_err == min(Total_cv_err));
lambda_best = lambda(flag);
w_final = RidgeRegress(x_train, y_train, lambda(7));
y_final =  x_test * w_final ;
mse_final = MSE(y_final, y_test);

%Plot

hold on
x = log(lambda);
plot(lambda, train_mse, 'r');
plot(lambda, test_mse, 'b');
plot(lambda_best, MSE_final,'*p')
legend('Trainning Error', 'Testing Error', )
xlabel('Lambda')
ylabel('MSE');
title('Error Against Regularization Parameters')
hold off









